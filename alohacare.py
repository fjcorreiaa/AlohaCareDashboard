# -*- coding: utf-8 -*-
"""AlohaCare.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cWulzID5z5s7hYPsDi29JHVE5LOe0Pff

#Preparing the environment
   
*   Load data
*   Import libraries
*   Analyze missing data and adjustments
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
import gc  # Para coleta de lixo e otimização de memória
from sklearn.decomposition import PCA

# Dataset Members and Enrollment
df_members = pd.read_csv('/content/drive/MyDrive/Lokahi_Innovation_in_Healthcare_Hackahton/df_final.csv')
df = pd.read_csv('/content/drive/MyDrive/Lokahi_Innovation_in_Healthcare_Hackahton/Claims_Services/combined_data_services.csv', low_memory=False)

# keep only one line in df with the same PRIMARY_PERSON_KEY

df = df.drop_duplicates(subset=['PRIMARY_PERSON_KEY'], keep='first')

# keep rows of column SV_STAT = P or D in df
df = df[df['SV_STAT'].isin(['P', 'D'])]

#Missing Data
# Calculate the percentage of NaN values for each column in merged_df
nan_percentages = df.isnull().sum() / len(df) * 100

# Create a DataFrame from the percentages
nan_percentage_df = pd.DataFrame({'Column': nan_percentages.index, 'NaN Percentage': nan_percentages.values})

# Display the table with all rows
pd.set_option("display.max_rows", None)  # Show all rows
print(nan_percentage_df)
pd.reset_option("display.max_rows")  # Reset to default

# Keep important columns in df, and with less missing
columns_to_keep = [
    "PRIMARY_PERSON_KEY", "AGE_ON_DOS", "SV_STAT", "RELATION", "PAYER_LOB", "PAYER_TYPE",
    "MEM_STAT", "SERVICE_SETTING", "FORM_TYPE", "PROC_CODE",
    "ICD_DIAG_ADMIT", "RX_DRUG_COST", "RX_QTY_DISPENSED",
    "AMT_BILLED", "AMT_PAID",
    "DIAGNOSTIC_CONDITION_CATEGORY_ID", "DIAGNOSTIC_CONDITION_CATEGORY_DESC", "ICD_DIAG_ADMIT", "DIAG_CCS_1_LABEL", "CPT_CCS_LABEL", "ICD_DIAG_01"
]

df2 = df[columns_to_keep]  # Manter apenas as colunas especificadas

# Exibir as colunas do dataframe resultante
print(f"Colunas no dataframe filtrado: {df2.columns.tolist()}")

# Remove rows where 'CPT_CCS_LABEL' is empty
df2 = df2.dropna(subset=['CPT_CCS_LABEL'])

# Join df_members and df members based on PRIMARY_PERSON_KEY

# Merge the two dataframes based on 'PRIMARY_PERSON_KEY'
merged_df = pd.merge(df_members, df2, on='PRIMARY_PERSON_KEY', how='left')

# Display the first few rows of the merged dataframe
merged_df.head()

# Remove columns 'PRIMARY_PERSON_KEY' and 'MEMBER_ID_x'
merged_df = merged_df.drop(columns=['PRIMARY_PERSON_KEY', 'MEMBER_ID_x'], errors='ignore')

"""Adding new variables based on those obtained in df_merged"""

# Generate the GROUP_COST column, to identify people with low, medium and high expenses. Values ​​based on the 25%, 50% and 75% percentiles

def classify_cost_numeric(amt):
    if amt <= 71:
        return 1  # Low spend
    elif 71 < amt < 194:
        return 2  # Medium spend
    else:
        return 3  # High spend

# Mapeamento para rótulos
label_mapping = {
    1: "Low spend",
    2: "Medium spend",
    3: "High spend"
}

# Gerar a coluna numérica GRUPO_CUSTO
merged_df["GROUP_COST"] = merged_df["AMT_BILLED"].apply(classify_cost_numeric)

# Adicionar a coluna de rótulos LABEL_GRUPO_CUSTO
merged_df["LABEL_GROUP_COST"] = merged_df["GROUP_COST"].map(label_mapping)

# Get table from https://raw.githubusercontent.com/k4m1113/ICD-10-CSV/master/categories.csv
# do not add first line as table name. Column 1= ICD, Column 2=Description

import pandas as pd

# URL do arquivo CSV
url = 'https://raw.githubusercontent.com/k4m1113/ICD-10-CSV/master/categories.csv'

# Ler o arquivo CSV diretamente da URL, pulando a primeira linha (cabeçalho)
icd_df = pd.read_csv(url, skiprows=1, header=None, names=['ICD', 'Description'])

# Exibir o DataFrame
icd_df

# Perform the merge to add the descriptions
merged_df = merged_df.merge(icd_df, left_on="ICD_DIAG_01", right_on="ICD", how="left")
merged_df["Description"] = merged_df["Description"].fillna("Código inválido")
merged_df.drop(columns=["ICD"], inplace=True)

# prompt: CRIAR FAMILIA_CID  COM BASE NAS 3 PRIMEIRAS linhas de ICD_DIAG_01

# Extract the first three characters of 'ICD_DIAG_01'
merged_df['FAMILIA_CID'] = merged_df['ICD_DIAG_01'].str[:3]

# Remove columns 'PRIMARY_PERSON_KEY', 'MEMBER_ID_x', and 'MEM_ZIP3'
merged_df = merged_df.drop(columns=['PRIMARY_PERSON_KEY', 'MEMBER_ID_x', 'MEM_ZIP3_x', 'MEM_ZIP3_y'], errors='ignore')

# Transformar as colunas em categóricas mantendo os valores ausentes
categorical_columns = ["MEM_RACE_x", "MEM_ETHNICITY_x"]
for col in categorical_columns:
    merged_df[col] = merged_df[col].astype("category")

# Exibir informações das colunas para verificar
print("Informações do DataFrame:")
print(merged_df.info())

# Remove columns 'MEM_RACE_y', 'MEM_ETHNICITY_y', 'MEMBER_ID_y', 'MEM_MSA_NAME_y', 'MEM_STATE_y', and 'PAYER_LOB_y'
merged_df = merged_df.drop(columns=['MEM_RACE_y', 'MEM_ETHNICITY_y', 'MEMBER_ID_y', 'MEM_MSA_NAME_y', 'MEM_STATE_y', 'PAYER_LOB_y', 'MEM_GENDER_y'], errors='ignore')

# Convert 'MEM_AGE' and 'AGE_ON_DOS' to numeric, handling errors
merged_df['MEM_AGE'] = pd.to_numeric(merged_df['MEM_AGE'], errors='coerce')
merged_df['AGE_ON_DOS'] = pd.to_numeric(merged_df['AGE_ON_DOS'], errors='coerce')

# prompt: gera CAtegorias de idade em merge_df com base em AGE_ON_DOS, considerar as categorias até 18 anos, 18 a 60 anos, 60 anos ou mais

def categorize_age(age):
    if age < 18:
        return '0-18'
    elif 18 < age < 60:
        return '18-60'
    else:
        return '60+'

merged_df['AGE_CATEGORY'] = merged_df['AGE_ON_DOS'].apply(categorize_age)

"""# Initial description of numerical and categorical variables"""

numeric_vars = [ 'RX_DRUG_COST',
 'AMT_BILLED',
 'AMT_PAID',
 'GROUP_COST',
 'AGE_ON_DOS', 'MEM_AGE']

categorical_vars = [
    "MEM_GENDER_x", "MEM_RACE_x", "PAYER_LOB_x", "SV_STAT", "RELATION",
    "SERVICE_SETTING", "DIAGNOSTIC_CONDITION_CATEGORY_DESC", "CPT_CCS_LABEL", "LABEL_GROUP_COST", "FAMILIA_CID", "AGE_CATEGORY"
]

for col in categorical_vars:
    if col in merged_df.columns:  # Verifica se a coluna existe no DataFrame
        merged_df[col] = merged_df[col].astype("category")

# Summary of numerical variables

for var in numeric_vars:
  print(f"Summary for {var}:")
  print(merged_df[var].describe())
  print("-" * 20)

# Calculate percentage of each category in categorical variables
for col in categorical_vars:
    print(f"\nPercentage distribution for {col}:")
    print(merged_df[col].value_counts(normalize=True) * 100)

# prompt: criar novo df com base em merged_df que exclua as linhas de DIAGNOSTIC_CONDITION_CATEGORY_DESC que contenha "NOT MAPPED " e  "QUESTIONABLE"

# Create a new DataFrame excluding rows wit
df_final = merged_df[
    ~merged_df["DIAGNOSTIC_CONDITION_CATEGORY_DESC"].str.contains("NOT MAPPED|QUESTIONABLE", na=False)
]

# prompt: remover merged_df, df df2

del merged_df
del df
del df2
gc.collect() #Force garbage collection

"""# Clusters"""

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer

numeric_data = df_final[numeric_vars]

# Step 2: Handle missing values (Imputation)
imputer = SimpleImputer(strategy="mean")

# Imputar valores ausentes
numeric_data_imputed = pd.DataFrame(
    imputer.fit_transform(numeric_data),  # Imputação
    columns=numeric_data.columns,  # Manter os nomes das colunas
    index=numeric_data.index       # Manter os índices originais
)

# Substituir os dados originais no DataFrame final
df_final[numeric_vars] = numeric_data_imputed

def fill_missing_with_multiple_modes(df_final, categorical_vars, max_modes=15):
    for col in categorical_vars:
        # Obter até max_modes valores de moda
        modes = df_final[col].mode().iloc[:max_modes]
        missing_indices = df_final[col].isna()

        # Preencher os valores ausentes iterativamente com as modas
        for mode in modes:
            if missing_indices.sum() == 0:  # Verifica se não há mais valores ausentes
                break
            df_final.loc[missing_indices, col] = mode
            missing_indices = df_final[col].isna()  # Atualizar os índices ausentes
    return df_final
df_final = fill_missing_with_multiple_modes(df_final, categorical_vars, max_modes=15)

for col in categorical_vars:
    if col in df_final.columns:  # Verifica se a coluna existe no DataFrame
        df_final[col] = df_final[col].astype("category")

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Pré-processamento: Codificar variáveis categóricas
df_encoded = pd.get_dummies(df_final, columns=categorical_vars, drop_first=True)

# 2. Garantir que variáveis numéricas estão em df_encoded
for col in numeric_vars:
    if col not in df_encoded.columns:
        df_encoded[col] = df_final[col]

#garantir vars

# Assuming df_final is your DataFrame and df_encoded will be the new one
df_encoded = df_final[categorical_vars + numeric_vars].copy()


# 3. Escalar as variáveis numéricas
numeric_data = df_encoded[numeric_vars]
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)

# 4. Aplicar o K-Means
kmeans = KMeans(n_clusters=2, random_state=42)
df_encoded["Cluster"] = kmeans.fit_predict(scaled_data)

# 5. Adicionar os clusters ao dataframe original
df_final["Cluster"] = df_encoded["Cluster"]

# 6. Visualização dos clusters
plt.figure(figsize=(10, 7))
sns.scatterplot(
    x="AGE_ON_DOS",  # Nome da coluna deve estar correto
    y="AMT_BILLED",  # Nome da coluna deve estar correto
    hue="Cluster",
    data=df_final,
    palette="Set2",
    style="Cluster"
)
plt.title("Clusters Formados pelo K-Means")
plt.xlabel("Age")  # Rotular corretamente
plt.ylabel("Billed Amount")  # Rotular corretamente
plt.show()

# 6. Analisar os clusters
print("Número de amostras por cluster:")
print(df_final["Cluster"].value_counts())

mean_by_cluster = df_final.groupby("Cluster")[numeric_vars].mean()

# Exibir o resultado
print("\nMédia de cada variável por cluster:")
print(mean_by_cluster)

cross_tabs = {}
for var in categorical_vars:
    cross_tabs[var] = pd.crosstab(df_final["Cluster"], df_final[var])

# Display cross-tabulations
for var, crosstab in cross_tabs.items():
    print(f"\nCross-tabulation for {var}:")
    print(crosstab)

import pandas as pd
import numpy as np
from scipy.stats import f_oneway, chi2_contingency

# 1. Compute p-values for numeric variables (ANOVA)
anova_results = {}
for var in numeric_vars:
    grouped = [df_final[df_final["Cluster"] == cluster][var] for cluster in df_final["Cluster"].unique()]
    _, p_value = f_oneway(*grouped)
    anova_results[var] = round(p_value, 6)  # Round to 6 decimal places

# 2. Compute p-values for categorical variables (Chi-Square)
chi2_results = {}
for var in categorical_vars:
    contingency_table = pd.crosstab(df_final["Cluster"], df_final[var])
    _, p_value, _, _ = chi2_contingency(contingency_table)
    chi2_results[var] = round(p_value, 6)  # Round to 6 decimal places

# Combine results
p_values = {**anova_results, **chi2_results}

# Display results as a list
print("P-values for comparisons between Clusters and Variables:")
for var, p_val in p_values.items():
    print(f"{var}: {p_val}")

print(type(df_encoded))  # Deve ser pandas.DataFrame ou numpy.ndarray
print(df_encoded.shape)  # Confirme o número de linhas e colunas

#from sklearn.cluster import KMeans
#import matplotlib.pyplot as plt

# Calcular inércia para diferentes números de clusters
#inertia = []
#K = range(1, 10)  # Testar de 1 a 10 clusters
#for k in K:
#    kmeans = KMeans(n_clusters=k, random_state=42)
#    kmeans.fit(df_final)
#    inertia.append(kmeans.inertia_)

# Plotar o gráfico do método do cotovelo
#plt.figure(figsize=(8, 5))
#plt.plot(K, inertia, 'bx-')
#plt.xlabel('Número de Clusters (k)')
#plt.ylabel('Inércia')
#plt.title('Método do Cotovelo para Determinar k Ideal')
#plt.show()

from sklearn.decomposition import PCA

# Reduzir para 2 componentes principais
#pca = PCA(n_components=2)
#principal_components = pca.fit_transform(df_final)

# Plotar os clusters atuais
#plt.figure(figsize=(8, 6))
#plt.scatter(principal_components[:, 0], principal_components[:, 1], cmap="viridis", s=50)
#plt.title('Visualização dos Clusters com PCA')
#plt.xlabel('Componente Principal 1')
#plt.ylabel('Componente Principal 2')
#plt.colorbar(label='Cluster')
#plt.show()

"""# Exportar dados"""

crosstab_data = {}
for var in categorical_vars:
    crosstab = pd.crosstab(df_final["Cluster"], df_final[var])
    crosstab_data[var] = crosstab.to_dict()

numeric_means = df_final.groupby("Cluster")[numeric_vars].mean().to_dict()

import json

# Dados do JSON
clusters_list = {
    "0": "Paciente com baixo risco de gasto",
    "1": "Paciente com alto risco de gasto"
}

json_data = {
    "clusters": clusters_list,
    "crosstab": crosstab_data,
    "numeric_means": numeric_means
}

# Salvar no JSON
with open("cluster_data.json", "w") as f:
    json.dump(json_data, f, indent=4)

"""# Streamlite Vision"""

import plotly.express as px
pip install streamlit
import streamlit as st

clusters_list = {
    "0": "Paciente com baixo risco de gasto",
    "1": "Paciente com alto risco de gasto"
}

cluster_group = clusters_list.get(str(df_final["Cluster"].iloc[0]), "Cluster desconhecido")
cluster_group

cluster_group = st.sidebar.selectbox('What is the patient group?', cluster_group)

colunas = ['AMT_BILLED','AGE_ON_DOS']
column = st.sidebar.selectbox('What type of information', colunas)


fig = px.histogram (df_final, y=column, title=column + cluster_group )
fig.update_layout( yaxis_title=column.upper(), title = {'x':0.5})

st.title('Patient Age and Spending, Hawaii 2024')
st.write('In this application, the user has the option of choosing the type of information to display the graph. Use the side menu to change the display.')
st.plotly_chart(fig, use_container_width=True)
